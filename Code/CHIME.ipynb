{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JHmc6Zas8Z2t",
        "TBdlMNx6dxo1",
        "12ga2b3nAxbA",
        "_ssM2A_ojGSz",
        "CHQKKXExaFzT",
        "VlDPHmNrettx",
        "lf0R1e1Q-ikn",
        "6zk63HdAaRGD",
        "ylMTh7pO6m6d",
        "j6_j4wobMgVp",
        "ijXz9RCkgwK2",
        "quQnmO_ciH_q",
        "zFgon7CZ5FPm",
        "9kmRoC0iTNX_",
        "abevbzP_kVrG",
        "6G5fnIx_7BvW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline"
      ],
      "metadata": {
        "id": "gEMaJoARvWU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount Google Drive"
      ],
      "metadata": {
        "id": "JHmc6Zas8Z2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va5JoEhiaq1x",
        "outputId": "aad154f0-b4ab-479d-cd7a-9285825a4ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/My Drive/Code Documentation Project/Hallucination/OpenJ9/' #ElasticSearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC-T0W9IawdY",
        "outputId": "bb7d6af9-cbe9-43e0-a311-071b32194f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Code Documentation Project/Hallucination/OpenJ9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load libraries\n"
      ],
      "metadata": {
        "id": "12ga2b3nAxbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install langchain-experimental\n",
        "!pip install langchainhub"
      ],
      "metadata": {
        "id": "v_fbUd0VAvW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, SQLDatabase\n",
        "from langchain_experimental.sql import SQLDatabaseChain\n",
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "import os\n",
        "import sqlite3\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate,FewShotChatMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.pydantic_v1 import BaseModel, Field"
      ],
      "metadata": {
        "id": "UkGnskhMA29a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ChatGPT\n",
        "\n"
      ],
      "metadata": {
        "id": "xSR3qc-_KHba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "\n",
        "# Initialize the OpenAI language model with a specified temperature\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Load two different SQLite databases from the provided URIs\n",
        "db = SQLDatabase.from_uri(\"sqlite:///Benchmark/OpenJ9_benchmark_issues_unstructured.db\")\n",
        "db_cfg = SQLDatabase.from_uri(\"sqlite:///Benchmark/OpenJ9_benchmark_issues_structured.db\")\n"
      ],
      "metadata": {
        "id": "wuzDu9d7I-rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def retriever(query, cfg=False):\n",
        "    \"\"\"\n",
        "    Retrieves data based on the provided query and configuration.\n",
        "\n",
        "    Parameters:\n",
        "    query (str): The SQL query to be executed.\n",
        "    cfg (bool): Flag to determine which database configuration to use.\n",
        "                If True, use the structured database. If False, use the unstructured database.\n",
        "\n",
        "    Returns:\n",
        "    str: The result of the query execution.\n",
        "    \"\"\"\n",
        "    if cfg:\n",
        "        # If cfg is True, use the structured database\n",
        "        print('Using structured database (cfg)')\n",
        "        db_chain_cfg = SQLDatabaseChain.from_llm(llm, db_cfg, verbose=True, use_query_checker=True, top_k=40)\n",
        "        return db_chain_cfg.run(query)\n",
        "    else:\n",
        "        # If cfg is False, use the unstructured database\n",
        "        print('Using unstructured database (normal)')\n",
        "        db_chain_normal = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True, top_k=40)\n",
        "        return db_chain_normal.run(query)"
      ],
      "metadata": {
        "id": "mgLYElc_hPjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query Preprocessor"
      ],
      "metadata": {
        "id": "_ssM2A_ojGSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot examples for Query Preprocessor\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Find unresolved issues with no activity in the last 6 months\",\n",
        "        \"output\": \"Select issue numbers of open issues with last activity date older than 6 months ago.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Suggest existing labels to tag issue 18608?\",\n",
        "        \"output\": \"List all existing labels and find suitable one for issue 18608 based on its content\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Is issue 18102 and 18669 similar?\",\n",
        "        \"output\": \"Compare the exceptions, stack traces, and descriptions of issues 18102 and 18669 to determine similarity.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Are there any issues similar to issue 18669?\",\n",
        "        \"output\": \"Identify issues with exceptions similar to those in issue 18669.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How many times did the internal grinder tests fail in issue 17852?\",\n",
        "        \"output\": \"Extract the number of internal grinder test failures mentioned in issue 17852.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "Tdfb9WVX2Tp8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define example messages for the prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),  # User's input message\n",
        "        (\"ai\", \"{output}\"),   # AI's response message\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a few-shot prompt template using the defined examples\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,  # List of example input-output pairs\n",
        ")\n",
        "\n",
        "# Define the main chat prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You need to extract query information from a database of issues.\n",
        "     Your task is to rephrase the question to make it more concise and direct, without altering its core intent or specificity.\n",
        "     Questions are of four intents: Yes/No, Fact, Summarization, and List.\n",
        "     1. Yes/No: Change 'is/are/have there issues' to check if there are any issues with the provided condition.\n",
        "     2. List: Change 'Find issues' to 'List issue numbers' with the provided condition.\n",
        "     3. Summarization: Summarize the contents from issue title, exceptions, body, and labels.\n",
        "     4. Fact: Extract the fact.\n",
        "     Retain keywords and intent as it is.\n",
        "     Follow these examples:\"\"\"),  # System message providing instructions on how to process the input\n",
        "\n",
        "    # Include few-shot examples for guidance\n",
        "    few_shot_prompt,\n",
        "\n",
        "    # New user question to be processed\n",
        "    (\"user\", \"{question}\"),\n",
        "])\n",
        "\n",
        "# Create a pipeline for generating responses from the prompt using ChatOpenAI and parsing the output\n",
        "question_gen = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
        "\n",
        "# Documentation:\n",
        "# - `ChatPromptTemplate.from_messages` is used to define the structure of the chat prompt, specifying how messages are formatted and handled.\n",
        "# - `FewShotChatMessagePromptTemplate` is used to incorporate example input-output pairs to guide the model in generating appropriate responses.\n",
        "# - The `system` message provides instructions for rephrasing questions to be more concise and direct while maintaining their core intent and specificity.\n",
        "# - The `ChatOpenAI` model is used to generate responses based on the defined prompt, with a temperature of 0 for deterministic output.\n",
        "# - `StrOutputParser` parses the output from the model to ensure it meets the required format."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNwhroHrjFAB",
        "outputId": "9673dd19-065e-4529-d24d-056726ed5c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chain of Verification - CoVe"
      ],
      "metadata": {
        "id": "6zk63HdAaRGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OpenAI Chat model with deterministic output (temperature=0)\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Define the input variables that will be used in the prompt template\n",
        "input_variables = [\"query\"]\n",
        "\n",
        "# Define the template for the base response prompt\n",
        "base_response_template = \"\"\"Question: {query} Answer:\"\"\"\n",
        "\n",
        "# Create a PromptTemplate instance using the input variables and the template\n",
        "base_response_prompt_template = PromptTemplate(\n",
        "    input_variables=input_variables,  # Variables that will be replaced in the template\n",
        "    template=base_response_template   # The template string with placeholders\n",
        ")\n",
        "\n",
        "# Create an LLMChain instance that combines the language model and the prompt template\n",
        "base_response_chain = LLMChain(\n",
        "    llm=llm,  # The language model to use for generating responses\n",
        "    prompt=base_response_prompt_template,  # The prompt template to format the input\n",
        "    output_key=base_response_output_key  # Key to store the output in the chain\n",
        ")\n",
        "\n",
        "# Documentation:\n",
        "# - `ChatOpenAI(temperature=0)` initializes the OpenAI language model with a temperature of 0 for consistent, deterministic output.\n",
        "# - `input_variables` specifies the variables that will be used in the prompt template. In this case, only \"query\".\n",
        "# - `PromptTemplate` is used to create a prompt format where the variable `query` will be inserted into the `base_response_template`.\n",
        "# - `base_response_chain` is an `LLMChain` that combines the language model and the prompt template to generate a response based on the input query.\n",
        "# - `base_response_output_key` specifies the key under which the output from the language model will be stored."
      ],
      "metadata": {
        "id": "VqkjUL7Xklvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961f1e78-4188-4ddc-f72c-dca541d89719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify(query):\n",
        "    \"\"\"\n",
        "    Verifies the response to a user query by generating and answering verification questions.\n",
        "\n",
        "    Args:\n",
        "    - query (str): The user's input query to be verified.\n",
        "\n",
        "    Returns:\n",
        "    - intermediate_result: The intermediate result from the verification process.\n",
        "    - verify_results_str (str): String containing the verification questions and their answers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define input variables for the prompt templates\n",
        "    input_variables = [\"query\"]\n",
        "    base_response_output_key = \"base_response\"\n",
        "\n",
        "    # Define the template for generating the base response\n",
        "    base_response_template = \"\"\"Question: {query} Answer:\"\"\"\n",
        "\n",
        "    # Create a PromptTemplate instance for the base response\n",
        "    base_response_prompt_template = PromptTemplate(\n",
        "        input_variables=input_variables,\n",
        "        template=base_response_template\n",
        "    )\n",
        "\n",
        "    # Create an LLMChain for generating the base response\n",
        "    base_response_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=base_response_prompt_template,\n",
        "        output_key=base_response_output_key\n",
        "    )\n",
        "\n",
        "    # Choose the database based on the `cfg` flag\n",
        "    if(cfg):\n",
        "        db = SQLDatabase.from_uri(\"sqlite:///Benchmark/OpenJ9_benchmark_issues_structured.db\")\n",
        "    else:\n",
        "        db = SQLDatabase.from_uri(\"sqlite:///Benchmark/OpenJ9_benchmark_issues_unstructured.db\")\n",
        "\n",
        "    # Define the database chain for normal queries\n",
        "    db_chain_normal = SQLDatabaseChain.from_llm(\n",
        "        llm,\n",
        "        db,\n",
        "        verbose=True,\n",
        "        use_query_checker=True,\n",
        "        top_k=40,\n",
        "        output_key=base_response_output_key\n",
        "    )\n",
        "\n",
        "    # Function to run queries using the normal database chain\n",
        "    def normal_retriever(query):\n",
        "        return db_chain_normal.run(query)\n",
        "\n",
        "    # Define the template for generating verification questions\n",
        "    plan_verifications_template = \"\"\"\n",
        "    Given the below Question and answer, generate a series of verification questions that test the factual claims in the original baseline response.\n",
        "    For example, if part of a longform model response contains the statement “The 2 exception types found in the issue report are java.io.EOFException, AssertionError”, then one possible\n",
        "    verification question to check those data could be “Can java.io.EOFException be found in existing issue reports? If there is an issue number and asking about this issue only, focus on it.”\n",
        "\n",
        "    Question: {query}\n",
        "    Answer: {base_response}\n",
        "\n",
        "    <fact in query and passage>, <verification question, generated by combining the query and the fact>\n",
        "\n",
        "    {format_instructions}\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the output model for verification questions\n",
        "    class PlanVerificationsOutput(BaseModel):\n",
        "        query: str = Field(description=\"The user's query\", default=\"\")\n",
        "        base_response: str = Field(description=\"The response to the user's query\", default=\"\")\n",
        "        facts_and_verification_questions: dict[str, str] = Field(\n",
        "            description=\"Facts (as the dictionary keys) extracted from the response and verification questions related to the query (as the dictionary values)\",\n",
        "            default=\"\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        # Create a PydanticOutputParser instance for parsing verification results\n",
        "        plan_verifications_output_parser = PydanticOutputParser(\n",
        "            pydantic_object=PlanVerificationsOutput\n",
        "        )\n",
        "\n",
        "        # Create a PromptTemplate for generating verification questions\n",
        "        plan_verifications_prompt_template = PromptTemplate(\n",
        "            input_variables=input_variables + [base_response_output_key],\n",
        "            template=plan_verifications_template,\n",
        "            partial_variables={\n",
        "                \"format_instructions\": plan_verifications_output_parser.get_format_instructions()\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Create an LLMChain for generating verification questions\n",
        "        plan_verifications_chain = LLMChain(\n",
        "            llm=llm,\n",
        "            prompt=plan_verifications_prompt_template,\n",
        "            output_key=\"output\",\n",
        "            output_parser=plan_verifications_output_parser,\n",
        "        )\n",
        "\n",
        "        # Create a SequentialChain to combine the database chain and the verification chain\n",
        "        answer_and_plan_verification = SequentialChain(\n",
        "            chains=[db_chain_normal, plan_verifications_chain],\n",
        "            input_variables=[\"query\"],\n",
        "            output_variables=[\"output\"],\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Run the SequentialChain to get the intermediate result\n",
        "        intermediate_result = answer_and_plan_verification.run(query)\n",
        "\n",
        "        # Extract the claimed facts and verification questions from the result\n",
        "        claimed_facts = list(intermediate_result.facts_and_verification_questions.keys())\n",
        "        verification_questions = list(\n",
        "            intermediate_result.facts_and_verification_questions.values()\n",
        "        )\n",
        "\n",
        "        # Initialize an empty string to collect verification results\n",
        "        verify_results_str = \"\"\n",
        "        verify_input_variables = [\"question\"]\n",
        "        verify_output_key = \"answer\"\n",
        "        verify_template = \"\"\"{question}\"\"\"\n",
        "\n",
        "        # Create a PromptTemplate for answering verification questions\n",
        "        verify_prompt_template = PromptTemplate(\n",
        "            input_variables=verify_input_variables,\n",
        "            template=verify_template\n",
        "        )\n",
        "\n",
        "        # Create a SQLDatabaseChain for answering verification questions\n",
        "        verify_chain = SQLDatabaseChain.from_llm(\n",
        "            llm,\n",
        "            db,\n",
        "            verbose=True,\n",
        "            use_query_checker=True,\n",
        "            output_key=verify_output_key\n",
        "        )\n",
        "\n",
        "        # Answer each verification question and compile results\n",
        "        for i in range(len(verification_questions)):\n",
        "            claimed_fact = claimed_facts[i]\n",
        "            question = verification_questions[i]\n",
        "            answer = verify_chain.run(question)\n",
        "            answer = answer.lstrip(\"\\n\")\n",
        "            verify_results_str += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "\n",
        "        return intermediate_result, verify_results_str\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle exceptions and return default values\n",
        "        return None, ''"
      ],
      "metadata": {
        "id": "UcaSyG_9aQfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Cove(query):\n",
        "    \"\"\"\n",
        "    Revises the response to a query based on verification results to ensure consistency.\n",
        "\n",
        "    Args:\n",
        "    - query (str): The user's input query to be verified and answered.\n",
        "\n",
        "    Returns:\n",
        "    - final_response (str): The revised response that is consistent with the verified information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get the intermediate result and verification results for the query\n",
        "        intermediate_result, verify_results_str = verify(query)\n",
        "\n",
        "        # Check if intermediate result is None, indicating an issue with verification\n",
        "        if intermediate_result is None:\n",
        "            return ''\n",
        "\n",
        "        # Define input variables and template for generating the final response\n",
        "        final_response_input_variables = [\"query\", \"base_response\", \"verify_results\"]\n",
        "        final_response_template = \"\"\"Given the ORIGINAL_QUESTION and the ORIGINAL_RESPONSE,\n",
        "        revise the ORIGINAL_RESPONSE (if applicable) such that it is consistent with information in VERIFIED_SOURCE as answer for ORIGINAL_QUESTION.\n",
        "        Only keep consistent information.\n",
        "\n",
        "        <ORIGINAL_QUESTION>\n",
        "        {query}\n",
        "\n",
        "        <ORIGINAL_RESPONSE>\n",
        "        {base_response}\n",
        "\n",
        "        <VERIFIED_SOURCE>\n",
        "        {verify_results}\n",
        "\n",
        "        Final response:\n",
        "        \"\"\"\n",
        "\n",
        "        # Create a PromptTemplate instance for generating the final response\n",
        "        final_response_prompt_template = PromptTemplate(\n",
        "            input_variables=final_response_input_variables,\n",
        "            template=final_response_template,\n",
        "        )\n",
        "\n",
        "        # Create an LLMChain instance for generating the final response\n",
        "        final_response_chain = LLMChain(llm=llm, prompt=final_response_prompt_template)\n",
        "\n",
        "        # Generate the final response using the LLMChain\n",
        "        final_response = final_response_chain.run(\n",
        "            query=intermediate_result.query,\n",
        "            base_response=intermediate_result.base_response,\n",
        "            # Use verification results to revise the response\n",
        "            verify_results=verify_results_str,\n",
        "        )\n",
        "\n",
        "        # Print the final response for debugging or logging purposes\n",
        "        print(final_response)\n",
        "\n",
        "        return final_response\n",
        "    except Exception as e:\n",
        "        # Return an empty string if an exception occurs\n",
        "        return ''\n"
      ],
      "metadata": {
        "id": "PjVOQqJCGU_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metamorphic Testing - MT"
      ],
      "metadata": {
        "id": "ylMTh7pO6m6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def mutate_query(original_query, n_mutations=3):\n",
        "    \"\"\"\n",
        "    Generates multiple variations of a given query while retaining its semantic meaning.\n",
        "\n",
        "    Args:\n",
        "    - original_query (str): The original question that needs to be mutated.\n",
        "    - n_mutations (int): The number of different ways to mutate the original query (default is 3).\n",
        "\n",
        "    Returns:\n",
        "    - List[str]: A list of mutated queries.\n",
        "    \"\"\"\n",
        "    # Template for generating query mutations\n",
        "    template = \"\"\"\n",
        "    Generate {n_mutations} different ways to ask the following question, keeping the semantic meaning the same:\\n\\n'{input}'\\n\\nMutations:\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the ChatOpenAI model with a deterministic temperature setting\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "    # Create a PromptTemplate instance with the mutation prompt template\n",
        "    prompt_template = PromptTemplate.from_template(template=template.format(n_mutations=n_mutations))\n",
        "\n",
        "    # Create an LLMChain instance using the model and prompt template\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "    # Generate mutations using the chain\n",
        "    response = chain.predict(input=original_query)\n",
        "    print(response)  # For debugging or logging purposes\n",
        "\n",
        "    # Extract and return the mutated queries\n",
        "    # Assuming the response is a string, split it by newlines\n",
        "    mutated_queries = response.split(\"\\n\")  # Adjust this based on the actual format of 'response'\n",
        "\n",
        "    # Filter out empty lines or irrelevant parts\n",
        "    mutated_queries = [query.strip() for query in mutated_queries if query.strip() and not query.strip().startswith(\"Mutations\")]\n",
        "\n",
        "    return mutated_queries\n"
      ],
      "metadata": {
        "id": "H1snseUQ6pmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MT_answer(original_query, init_response):\n",
        "    \"\"\"\n",
        "    Processes an initial response to a query by generating variations of the query,\n",
        "    retrieving answers for each variation, and then revising the initial response based on verified information.\n",
        "\n",
        "    Args:\n",
        "    - original_query (str): The original question for which responses are being verified.\n",
        "    - init_response (str): The initial response to the original query that may need revision.\n",
        "\n",
        "    Returns:\n",
        "    - str: The final revised response that is consistent with verified information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate multiple mutations of the original query\n",
        "    mutated_queries = mutate_query(original_query)\n",
        "    print(mutated_queries)  # For debugging purposes\n",
        "\n",
        "    # Initialize an empty string to collect verification results\n",
        "    verify_results_str = \"\"\n",
        "\n",
        "    # Retrieve answers for each mutated query and collect verification information\n",
        "    for q in mutated_queries:\n",
        "        a = retriever(q)  # Use the retriever function to get answers for the mutated queries\n",
        "        verify_results_str += f\"Question: {q}\\nAnswer: {a}\\n\\n\"\n",
        "\n",
        "    # Define the original query and the initial response\n",
        "    query = original_query\n",
        "    base_response = init_response\n",
        "\n",
        "    # Print the base response and verification results for debugging\n",
        "    print('BASE--------')\n",
        "    print(base_response)\n",
        "    print('verify-----')\n",
        "    print(verify_results_str)\n",
        "\n",
        "    # Define the final response prompt template\n",
        "    final_response_input_variables = [\"query\", \"base_response\", \"verify_results\"]\n",
        "    final_response_template = \"\"\"Given the ORIGINAL_QUESTION and the ORIGINAL_RESPONSE,\n",
        "    revise the ORIGINAL_RESPONSE (if applicable) such that it is consistent with information in VERIFIED_SOURCE as answer for ORIGINAL_QUESTION.\n",
        "    Only keep consistent information.\n",
        "\n",
        "    <ORIGINAL_QUESTION>\n",
        "    {query}\n",
        "\n",
        "    <ORIGINAL_RESPONSE>\n",
        "    {base_response}\n",
        "\n",
        "    <VERIFIED_SOURCE>\n",
        "    {verify_results}\n",
        "\n",
        "    Final response:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a PromptTemplate instance for generating the final response\n",
        "    final_response_prompt_template = PromptTemplate(\n",
        "        input_variables=final_response_input_variables,\n",
        "        template=final_response_template,\n",
        "    )\n",
        "\n",
        "    # Create an LLMChain instance for generating the final response\n",
        "    final_response_chain = LLMChain(llm=llm, prompt=final_response_prompt_template)\n",
        "\n",
        "    # Generate the final response by running the LLMChain\n",
        "    final_response = final_response_chain.run(\n",
        "        query=query,\n",
        "        base_response=base_response,\n",
        "        verify_results=verify_results_str,\n",
        "    )\n",
        "\n",
        "    # Print the final response for debugging purposes\n",
        "    print('final-----')\n",
        "    print(\"Final Response:\", final_response)\n",
        "\n",
        "    return final_response\n"
      ],
      "metadata": {
        "id": "a8IYDJYg-Rlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configuration"
      ],
      "metadata": {
        "id": "VlDPHmNrettx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the name variable as an empty string\n",
        "name = ''\n",
        "\n",
        "# Flags for different configurations\n",
        "cfg = False\n",
        "change_query = False\n",
        "COVE = False\n",
        "MT = False\n",
        "\n",
        "# Append suffixes to name based on the configuration flags\n",
        "if cfg:\n",
        "    name += '_CFG'  # Append '_CFG' if cfg is True\n",
        "if change_query:\n",
        "    name += '_Query'  # Append '_Query' if change_query is True\n",
        "if COVE:\n",
        "    name += '_COVE'  # Append '_COVE' if COVE is True\n",
        "if MT:\n",
        "    name += '_MT'  # Append '_MT' if MT is True\n",
        "\n",
        "# Special case: if all flags are True, set name to '_CIMBUR'\n",
        "if cfg and change_query and COVE and MT:\n",
        "    name = '_CIMBUR'\n",
        "\n",
        "# Special case: if none of the flags are True, set name to '_LLM'\n",
        "if not (cfg or change_query or COVE or MT):\n",
        "    name = '_LLM'\n",
        "\n",
        "# Print the final value of name\n",
        "print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whzZzwSXeaDB",
        "outputId": "2e17e162-d2ca-4ee9-87c9-4d9ae9a6722e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_COVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def process_questions(questions):\n",
        "    # Initialize a list to store answers\n",
        "    answers = []\n",
        "\n",
        "    # Iterate over each question in the provided list\n",
        "    for i, q in enumerate(questions):\n",
        "        try:\n",
        "            # If change_query is enabled, generate a new query\n",
        "            if change_query:\n",
        "                print('Generating query')\n",
        "                q = question_gen.invoke({\"question\": q})\n",
        "\n",
        "            # If MT (Mutation Testing) is enabled\n",
        "            if MT:\n",
        "                if COVE:\n",
        "                    print('Running MT and COVE')\n",
        "                    # Use COVE to get initial response\n",
        "                    init_response = Cove(q)\n",
        "                else:\n",
        "                    # Retrieve initial response without COVE\n",
        "                    init_response = retriever(q)\n",
        "                # Get final answer using MT approach\n",
        "                answer = MT_answer(q, init_response)\n",
        "\n",
        "            # If only COVE is enabled\n",
        "            elif COVE:\n",
        "                print('Running COVE')\n",
        "                answer = Cove(q)\n",
        "\n",
        "            # Default case: use retriever\n",
        "            else:\n",
        "                answer = retriever(q)\n",
        "\n",
        "            # Append the answer to the list\n",
        "            answers.append(answer)\n",
        "            print(f'-----------Final-------------: {answer}\\n')\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any exceptions that occur\n",
        "            print(f\"An error occurred while processing question {i+1}: {e}\")\n",
        "            answers.append(None)  # Append None if an error occurs or question is skipped\n",
        "\n",
        "    # Return the list of answers\n",
        "    return answers\n"
      ],
      "metadata": {
        "id": "AnsS3fy3KkDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Benchmark"
      ],
      "metadata": {
        "id": "uZFtdFQ3vKp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5 - Issue Backlog"
      ],
      "metadata": {
        "id": "gqj73gCR2-oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "lIE2mt__Ma8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('Benchmark/new/O-T5.csv')\n",
        "\n",
        "# Extract non-null questions for processing\n",
        "single_q = df.loc[df['T5-YNQ'].notnull(), 'T5-YNQ']\n",
        "\n",
        "# Process questions to get answers\n",
        "single_a = process_questions(single_q)\n",
        "\n",
        "# Add a new column for answers\n",
        "df[f'T5-YNA{name}'] = ''\n",
        "df.loc[:len(single_a) - 1, f'T5-YNA{name}'] = single_a\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df.to_csv('Benchmark/new/O-T5.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers from the DataFrame\n",
        "actual_answers = df.loc[df[f'T5-YNA{name}'].notnull(), f'T5-YNA{name}']\n",
        "expected_answers = df.loc[df['T5-YNA'].notnull(), 'T5-YNA']\n",
        "\n",
        "# Call the function to classify and evaluate answers\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "# Add a new column for evaluation results\n",
        "df[f'T5-YN{name}_R'] = ''\n",
        "df[f'T5-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "df.to_csv('Benchmark/new/O-T5.csv', index=False)"
      ],
      "metadata": {
        "id": "J7ySzbhZKWmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "j6_j4wobMgVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T5.csv')\n",
        "\n",
        "single_q = df.loc[df['T5-SQ'].notnull(), 'T5-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T5-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T5-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T5.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T5-SA'].notnull(), 'T5-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T5-S{name}_R'] = ''\n",
        "df[f'T5-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T5.csv', index=False)"
      ],
      "metadata": {
        "id": "kmYOj7FTMmKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "YAyeF9x7Mh3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T5.csv')\n",
        "\n",
        "single_q = df.loc[df['T5-FQ'].notnull(), 'T5-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T5-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T5-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T5.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T5-FA'].notnull(), 'T5-FA']\n",
        "expected_answers = expected_answers.astype(str)\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_fact(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T5-FA{name}_R'] = ''\n",
        "df[f'T5-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T5.csv', index=False)"
      ],
      "metadata": {
        "id": "552b6s0ONGgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##T4 - Issue Labelling"
      ],
      "metadata": {
        "id": "wvb4KMPBgs5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "ijXz9RCkgwK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T4.csv')\n",
        "\n",
        "single_q = df.loc[df['T4-YNQ'].notnull(), 'T4-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T4-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T4-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T4.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T4-YNA{name}'].notnull(), f'T4-YNA{name}']\n",
        "expected_answers = df.loc[df['T4-YNA'].notnull(), 'T4-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T4-YN{name}_R'] = ''\n",
        "df[f'T4-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T4.csv', index=False)"
      ],
      "metadata": {
        "id": "qeHMnVd9gwK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "quQnmO_ciH_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T4.csv')\n",
        "\n",
        "single_q = df.loc[df['T4-SQ'].notnull(), 'T4-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T4-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T4-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T4.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T4-SA'].notnull(), 'T4-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T4-S{name}_R'] = ''\n",
        "df[f'T4-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T4.csv', index=False)\n",
        "sum_correct=correct"
      ],
      "metadata": {
        "id": "HhwfGN2-iH_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "skwRQd6Vh9Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T4.csv')\n",
        "\n",
        "single_q = df.loc[df['T4-FQ'].notnull(), 'T4-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T4-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T4-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T4.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T4-FA'].notnull(), 'T4-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_fact(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T4-FA{name}_R'] = ''\n",
        "df[f'T4-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T4.csv', index=False)"
      ],
      "metadata": {
        "id": "G2BrDw-lh9G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##T3 - Issue Summary\n"
      ],
      "metadata": {
        "id": "zFgon7CZ5FPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "qpW1n7AFyEOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T3.csv')\n",
        "\n",
        "single_q = df.loc[df['T3-SQ'].notnull(), 'T3-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T3-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T3-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T3.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T3-SA'].notnull(), 'T3-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T3-S{name}_R'] = ''\n",
        "df[f'T3-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "BkcvcckGyEOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "wgW8-Y2hDGBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T3.csv')\n",
        "\n",
        "single_q = df.loc[df['T3-FQ'].notnull(), 'T3-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T3-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T3-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T3.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T3-FA'].notnull(), 'T3-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_fact(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T3-FA{name}_R'] = ''\n",
        "df[f'T3-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "T2qF4QrODGBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##T2 - Issue Trend"
      ],
      "metadata": {
        "id": "9kmRoC0iTNX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "S6QvEsvWTUuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T2.csv')\n",
        "\n",
        "single_q = df.loc[df['T2-YNQ'].notnull(), 'T2-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T2-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T2-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T2.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T2-YNA{name}'].notnull(), f'T2-YNA{name}']\n",
        "expected_answers = df.loc[df['T2-YNA'].notnull(), 'T2-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T2-YN{name}_R'] = ''\n",
        "df[f'T2-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "IfLPcRtLTUuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "YjqXdHXBTUuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T2.csv')\n",
        "\n",
        "single_q = df.loc[df['T2-SQ'].notnull(), 'T2-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T2-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T2-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T2.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T2-SA'].notnull(), 'T2-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T2-S{name}_R'] = ''\n",
        "df[f'T2-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T2.csv', index=False)"
      ],
      "metadata": {
        "id": "jZFh1IHITUuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "_j3MjoVzTUuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T2.csv')\n",
        "\n",
        "single_q = df.loc[df['T2-FQ'].notnull(), 'T2-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T2-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T2-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T2.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T2-FA'].notnull(), 'T2-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_fact(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T2-FA{name}_R'] = ''\n",
        "df[f'T2-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "gT1K57PqTUuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##T1 - Issue Analysis (Multiple)"
      ],
      "metadata": {
        "id": "abevbzP_kVrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "N0imq_TCkVrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T1M.csv')\n",
        "\n",
        "single_q = df.loc[df['T1M-YNQ'].notnull(), 'T1M-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1M-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1M-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1M.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T1M-YNA{name}'].notnull(), f'T1M-YNA{name}']\n",
        "expected_answers = df.loc[df['T1M-YNA'].notnull(), 'T1M-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1M-YN{name}_R'] = ''\n",
        "df[f'T1M-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1M.csv', index=False)\n"
      ],
      "metadata": {
        "id": "sHrnfYQokVrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "BgNL04V5kVrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T1M.csv')\n",
        "\n",
        "single_q = df.loc[df['T1M-SQ'].notnull(), 'T1M-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1M-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1M-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1M.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1M-SA'].notnull(), 'T1M-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1M-S{name}_R'] = ''\n",
        "df[f'T1M-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1M.csv', index=False)\n",
        "sum_correct=correct"
      ],
      "metadata": {
        "id": "BoVLjdBzkVrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "jLkcQjEPkVrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T1M.csv')\n",
        "\n",
        "single_q = df.loc[df['T1M-FQ'].notnull(), 'T1M-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1M-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1M-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1M.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1M-FA'].notnull(), 'T1M-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_fact(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1M-FA{name}_R'] = ''\n",
        "df[f'T1M-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1M.csv', index=False)\n"
      ],
      "metadata": {
        "id": "qcn0DgwikVrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##T1 - Issue Analysis (Single)"
      ],
      "metadata": {
        "id": "6G5fnIx_7BvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "x_7hu1-yxAX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T1S.csv')\n",
        "\n",
        "single_q = df.loc[df['T1S-YNQ'].notnull(), 'T1S-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1S-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1S-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1S.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T1S-YNA{name}'].notnull(), f'T1S-YNA{name}']\n",
        "expected_answers = df.loc[df['T1S-YNA'].notnull(), 'T1S-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "yn_correct=correct\n",
        "\n",
        "df[f'T1S-YN{name}_R'] = ''\n",
        "df[f'T1S-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1S.csv', index=False)\n"
      ],
      "metadata": {
        "id": "sZGxHqHt8Nlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1455871-334b-44e7-e632-fa82fcf87117",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "Does issue 18400 affect jit component ?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"number\", \"title\", \"labels\" \n",
            "FROM issues \n",
            "WHERE \"number\" = 18400;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(18400, 'Apache Lucene CI builds sometimes fail with OpenJ9 specific issues', 'comp:jit, userRaised, project:MH')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mYes, issue 18400 does affect the jit component.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: Yes, issue 18400 does affect the jit component.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "is issue 19014 an user raiser issue?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT issue_creator, labels \n",
            "FROM issues \n",
            "WHERE number = 19014;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[('TemporaryRepos', 'comp:jit, userRaised')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mYes, issue 19014 is a user-raised issue.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: Yes, issue 19014 is a user-raised issue.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "Is the issue 18082 found in windows machine?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"number\", \"title\", \"body\" \n",
            "FROM issues \n",
            "WHERE \"number\" = 18082;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(18082, 'aarch64_linux build test ant task failed with ArrayIndexOutOfBoundsException in some machines', 'Failure link\\r\\n------------\\r\\n\\r\\nFrom [an internal build](https://hyc-runtimes-jenkins.swg-devops.com/job/Test_openjdk21_j9_special.jck_aarch64_linux_Personal/29/console)(`cent7-aarch64-4`):\\r\\n```\\r\\n08:27:40  openjdk version \"21-internal\" 2023-09-19\\r\\n08:27:40  OpenJDK Runtime Environment (build...')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mNo, the issue 18082 is found in aarch64_linux machine, not in a windows machine.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: No, the issue 18082 is found in aarch64_linux machine, not in a windows machine.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "does issue 17774 require triage?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"number\", \"title\", \"state\" FROM issues WHERE \"number\" = 17774;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(17774, 'NullPointerExceptions seen in SpinedBufferTest', 'closed')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mNo, issue 17774 does not require triage as it is already closed.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: No, issue 17774 does not require triage as it is already closed.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "is component jit affected in issue 18043?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"number\", \"title\", \"labels\" \n",
            "FROM issues \n",
            "WHERE \"number\" = 18043;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(18043, 'FIPS JDK17 jdk_security2_0_FAILED javax/crypto/CipherSpi/CipherByteBufferOverwriteTest.java ERROR: Ciphertext does not match', 'test failure, comp:fips')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mYes, the component \"jit\" is affected in issue 18043.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: Yes, the component \"jit\" is affected in issue 18043.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "is issue 17540 can be found in jdk 17?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT number, title, state, created_at\n",
            "FROM issues\n",
            "WHERE number = 17540 AND repository_url LIKE '%jdk-17%'\n",
            "LIMIT 1;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mFinal answer here\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: Final answer here\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "is issue 17063 a linux issue?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT issue_creator, body\n",
            "FROM issues\n",
            "WHERE number = 17063;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[('pshipton', \"[Internal build](https://hyc-runtimes-jenkins.swg-devops.com/job/Test_openjdk8_j9_sanity.openjdk_s390x_linux/484/)\\r\\njdk_lang_1\\r\\njava/lang/String/ToLowerCase.java\\r\\nThe failure didn't repeat in the automatic 3x grinder.\\r\\n```\\r\\n[2023-03-29T02:29:30.273Z] STDERR:\\r\\n[2023-03-29T02:29:30.273Z]...\")]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mYes, issue 17063 is a linux issue.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: Yes, issue 17063 is a linux issue.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "is regression introduced in issue 17359?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT title, state, created_at, closed_at\n",
            "FROM issues\n",
            "WHERE number = 17359;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[('testGetSizeOfArray failed in Valhalla builds', 'closed', '2023-05-08', '2023-05-15')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mNo, regression is not introduced in issue 17359.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: No, regression is not introduced in issue 17359.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "does issue 18336 work fine on windows?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT state\n",
            "FROM issues\n",
            "WHERE number = 18336;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[('closed',)]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mNo, issue 18336 is closed.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: No, issue 18336 is closed.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "is there ExceptionInInitializerError in issue 18082?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT title, state, created_at, closed_at, issue_creator, labels, milestone, body\n",
            "FROM issues\n",
            "WHERE number = 18082;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[('aarch64_linux build test ant task failed with ArrayIndexOutOfBoundsException in some machines', 'closed', '2023-09-06', '2023-09-06', 'JasonFengJ9', 'test failure, arch:aarch64, triageRequired', None, 'Failure link\\r\\n------------\\r\\n\\r\\nFrom [an internal build](https://hyc-runtimes-jenkins.swg-devops.com/job/Test_openjdk21_j9_special.jck_aarch64_linux_Personal/29/console)(`cent7-aarch64-4`):\\r\\n```\\r\\n08:27:40  openjdk version \"21-internal\" 2023-09-19\\r\\n08:27:40  OpenJDK Runtime Environment (build...')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mNo, there is no ExceptionInInitializerError in issue 18082.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: No, there is no ExceptionInInitializerError in issue 18082.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "did the failure of issue 17063 repeat in automatic 3x grinder?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT * FROM issues WHERE title = '[Bug report] memoryPoolMXBean.getUsage will throw exception when gcpolicy is balanced';\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(17005, 'eclipse-openj9/openj9', '[Bug report] memoryPoolMXBean.getUsage will throw exception when gcpolicy is balanced', 'closed', 2, '2023-03-23', '2023-03-23', 'xiangtianyu', 'comp:gc, userRaised', '', 'Release 0.40 (Java 8, 11, 17, 20) July refresh', 'Java -version output\\r\\n--------------------\\r\\nI tried several versions and have the same problem.\\r\\n\\r\\n```\\r\\nopenjdk version \"17.0.4.1-ea\" 2022-08-12\\r\\nIBM Semeru Runtime Open Edition 17.0.4.1 (build 17.0.4.1-ea+1)\\r\\nEclipse OpenJ9 VM 17.0.4.1 (build openj9-0.33.1, JRE 17 Mac OS X aarch64-64-Bit...')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mNo, the failure of issue 17063 did not repeat in automatic 3x grinder.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: No, the failure of issue 17063 did not repeat in automatic 3x grinder.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "is issue 17852 a machine issue?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"number\", \"title\", \"labels\" \n",
            "FROM issues \n",
            "WHERE \"number\" = 17852;\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(17852, 'jdk_nio_1_FAILED java/nio/channels/FileChannel/ExpandingMap.java InternalError: SIGBUS', 'test failure')]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mNo, issue 17852 is not a machine issue.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----------Final-------------: No, issue 17852 is not a machine issue.\n",
            "\n",
            "Yes, issue 18400 does affect the jit component.\n",
            "Yes\n",
            "1\n",
            "Yes, issue 19014 is a user-raised issue.\n",
            "Yes\n",
            "1\n",
            "No, the issue 18082 is found in aarch64_linux machine, not in a windows machine.\n",
            "No\n",
            "1\n",
            "No, issue 17774 does not require triage as it is already closed.\n",
            "yes\n",
            "0\n",
            "Yes, the component \"jit\" is affected in issue 18043.\n",
            "No\n",
            "0\n",
            "Final answer here\n",
            "No\n",
            "0\n",
            "Yes, issue 17063 is a linux issue.\n",
            "yes\n",
            "1\n",
            "No, regression is not introduced in issue 17359.\n",
            "yes\n",
            "0\n",
            "No, issue 18336 is closed.\n",
            "Yes\n",
            "0\n",
            "No, there is no ExceptionInInitializerError in issue 18082.\n",
            "Yes\n",
            "0\n",
            "No, the failure of issue 17063 did not repeat in automatic 3x grinder.\n",
            "No\n",
            "1\n",
            "No, issue 17852 is not a machine issue.\n",
            "Yes\n",
            "0\n",
            "Correct 5\n",
            "Correctness: 0.4166666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-66da35720b8b>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[f'T1S-YN{name}_R'][:len(results)] = results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "n17tSjeVxGWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T1S.csv')\n",
        "\n",
        "single_q = df.loc[df['T1S-SQ'].notnull(), 'T1S-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1S-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1S-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1S.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1S-SA'].notnull(), 'T1S-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1S-S{name}_R'] = ''\n",
        "df[f'T1S-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1S.csv', index=False)\n"
      ],
      "metadata": {
        "id": "nxOdZJCGxFuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "4Kp9TMwB5qNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/new/O-T1S-F.csv')\n",
        "\n",
        "single_q = df.loc[df['T1S-FQ'].notnull(), 'T1S-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1S-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1S-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1S-F.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1S-FA'].notnull(), 'T1S-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_fact(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1S-FA{name}_R'] = ''\n",
        "df[f'T1S-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/new/O-T1S-F.csv', index=False)\n",
        "fact_correct=correct\n"
      ],
      "metadata": {
        "id": "Dn_C4kOb5zI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "jYZK8rnZZ4_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "n67SSfW6xUR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\")"
      ],
      "metadata": {
        "id": "GHBLggvjjtGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "def classify_and_evaluate(actual_answers, expected_answers):\n",
        "    \"\"\"\n",
        "    Classify actual answers using zero-shot classification and evaluate the results against expected answers.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_answers (list of str): List of answers to be classified.\n",
        "    - expected_answers (list of str): List of expected classifications.\n",
        "\n",
        "    Returns:\n",
        "    - results (list of int): List indicating correctness (1 for correct, 0 for incorrect).\n",
        "    - correct (int): Number of correctly classified answers.\n",
        "    - correctness (float): Proportion of correct classifications.\n",
        "    \"\"\"\n",
        "    # Define classes for classification\n",
        "    classes = [\"Yes\", \"No\"]\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    for actual, expected in zip(actual_answers, expected_answers):\n",
        "        print(actual)  # Print the actual answer for debugging\n",
        "        print(expected)  # Print the expected answer for debugging\n",
        "\n",
        "        total += 1\n",
        "        # Classify the actual answer\n",
        "        result = classifier(actual, candidate_labels=classes, hypothesis_template=\"This statement implies: {}.\")\n",
        "\n",
        "        # Check if the predicted label matches the expected label\n",
        "        if result['labels'][0].lower() == expected.lower():\n",
        "            correct += 1\n",
        "            results.append(1)\n",
        "            print(1)  # Debug output for correct classification\n",
        "        else:\n",
        "            results.append(0)\n",
        "            print(0)  # Debug output for incorrect classification\n",
        "\n",
        "    print('Correct ' + str(correct))  # Print the number of correct classifications\n",
        "    correctness = correct / total  # Calculate the proportion of correct classifications\n",
        "    return results, correct, correctness\n"
      ],
      "metadata": {
        "id": "H1R7EJt5bcIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "znbdOVdcxXOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "H7WbwsFJhTdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Initialize the model for semantic similarity\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def extract_numbers(text):\n",
        "    \"\"\"\n",
        "    Extracts all numeric values from the given text.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text from which numbers are to be extracted.\n",
        "\n",
        "    Returns:\n",
        "    - list of str: A list of numeric strings found in the text.\n",
        "    \"\"\"\n",
        "    return re.findall(r'\\b\\d+\\b', text)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by removing non-alphanumeric characters and excess whitespace.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "    - str: The cleaned text.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'[^\\w\\s:]', '', text)  # Remove punctuation except colons\n",
        "    text = text.strip().replace('\\n', '').replace('\"', '').replace(\"'\", '')\n",
        "    return text\n",
        "\n",
        "def containsOnlyNumbers(text):\n",
        "    \"\"\"\n",
        "    Checks if the cleaned text contains only numbers.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to be checked.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the text contains only numbers, False otherwise.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    text = text.replace('\\n', '').replace('and', '')\n",
        "\n",
        "    all_numbers = ' '.join(extract_numbers(text))\n",
        "\n",
        "    if text == all_numbers:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def evaluate_fact(actual_answers, expected_answers):\n",
        "    \"\"\"\n",
        "    Evaluates the correctness of answers by comparing them with expected answers using semantic similarity.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_answers (list of str): The list of actual answers to be evaluated.\n",
        "    - expected_answers (list of str): The list of expected answers for comparison.\n",
        "\n",
        "    Returns:\n",
        "    - results (list of int): List indicating correctness (1 for correct, 0 for incorrect).\n",
        "    - correct (int): Number of correct answers.\n",
        "    - correctness (float): Proportion of correct answers.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    correct = 0\n",
        "\n",
        "    for actual, expected in zip(actual_answers, expected_answers):\n",
        "\n",
        "        if actual is None and expected is None:\n",
        "            continue  # Skip if both are None\n",
        "\n",
        "        if actual is None:\n",
        "            actual = 'None'\n",
        "\n",
        "        expected = expected.strip() if expected else ''\n",
        "        actual = actual.strip() if actual else ''\n",
        "\n",
        "        similarity = 0\n",
        "\n",
        "        if expected.isdigit() or len(expected.split()) == 1:\n",
        "            # Case: expected answer is a number or single word\n",
        "            actual_s = set(clean_text(actual).split())\n",
        "            expected_s = set(clean_text(expected).split())\n",
        "\n",
        "            actual_s = set(word.lower() for word in actual_s)\n",
        "            expected_s = set(word.lower() for word in expected_s)\n",
        "\n",
        "            intersection = actual_s.intersection(expected_s)\n",
        "\n",
        "            if intersection == set(expected.split()) or intersection:\n",
        "                similarity = 1\n",
        "\n",
        "        elif containsOnlyNumbers(expected):\n",
        "            # Case: expected answer contains only numbers\n",
        "            expected_num = sorted(extract_numbers(expected))\n",
        "            expected_s = ' '.join(expected_num)\n",
        "            min_limit = min(expected_num)\n",
        "            max_limit = max(expected_num)\n",
        "\n",
        "            actual_num = sorted(extract_numbers(actual))\n",
        "            if actual_num:\n",
        "                actual_num = [num for num in actual_num if num > min_limit or num <= max_limit]\n",
        "                actual_s = ' '.join(actual_num)\n",
        "                actual_embedding = model.encode(actual_s, convert_to_tensor=True)\n",
        "                expected_embedding = model.encode(expected_s, convert_to_tensor=True)\n",
        "                similarity = util.pytorch_cos_sim(actual_embedding, expected_embedding).item()\n",
        "\n",
        "        else:\n",
        "            # Case: general text comparison\n",
        "            actual_embedding = model.encode(actual, convert_to_tensor=True)\n",
        "            expected_embedding = model.encode(expected, convert_to_tensor=True)\n",
        "            similarity = util.pytorch_cos_sim(actual_embedding, expected_embedding).item()\n",
        "\n",
        "        threshold = 0.7\n",
        "        is_correct = 1 if similarity >= threshold else 0\n",
        "        if is_correct == 1:\n",
        "            correct += 1\n",
        "        results.append(is_correct)\n",
        "\n",
        "    correctness = correct / len(results) if results else 0\n",
        "    return results, correct, correctness\n"
      ],
      "metadata": {
        "id": "pFFmplKXDkFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "SdRrOTvfxZYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "1D4zYBlozqFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Initialize the model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def evaluate_summary(actual_answers, expected_answers):\n",
        "    \"\"\"\n",
        "    Evaluates the correctness of actual answers compared to expected answers based on semantic similarity.\n",
        "\n",
        "    Args:\n",
        "        actual_answers (list of str): List of actual answers.\n",
        "        expected_answers (list of str): List of expected answers.\n",
        "\n",
        "    Returns:\n",
        "        results (list of int): List indicating correctness of each comparison (1 for correct, 0 for incorrect).\n",
        "        correct (int): Total number of correct comparisons.\n",
        "        correctness (float): Proportion of correct comparisons.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    correct = 0\n",
        "\n",
        "    # Evaluate correctness through semantic similarity\n",
        "    for actual, expected in zip(actual_answers, expected_answers):\n",
        "        # Handle None values\n",
        "        if actual is None:\n",
        "            actual = 'None'\n",
        "\n",
        "        # Encode answers to embeddings\n",
        "        actual_embedding = model.encode(actual, convert_to_tensor=True)\n",
        "        expected_embedding = model.encode(expected, convert_to_tensor=True)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarity = util.pytorch_cos_sim(actual_embedding, expected_embedding).item()\n",
        "\n",
        "        # Threshold for correctness (can be adjusted)\n",
        "        threshold = 0.7\n",
        "        is_correct = 1 if similarity >= threshold else 0\n",
        "\n",
        "        # Update count and results list\n",
        "        if is_correct == 1:\n",
        "            correct += 1\n",
        "        results.append(is_correct)\n",
        "\n",
        "        # Print debug information\n",
        "        print(f'Actual: {actual}')\n",
        "        print(f'Expected: {expected}')\n",
        "        print(f'Similarity: {similarity:.4f}')\n",
        "        print(f'Correct: {is_correct}')\n",
        "\n",
        "    # Calculate and print overall correctness\n",
        "    correctness = correct / len(results) if results else 0\n",
        "    print(f'Correct: {correct}')\n",
        "    print(f'Correctness: {correctness:.4f}')\n",
        "\n",
        "    return results, correct, correctness\n"
      ],
      "metadata": {
        "id": "n22dLsBbxyFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accuracy of CFG"
      ],
      "metadata": {
        "id": "aKP-o2KOvEvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class StackTraceParser:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the StackTraceParser with regex patterns for parsing stack traces.\n",
        "        \"\"\"\n",
        "        self.exception_pattern = r'^(?:Caused by: )?(?:Exception in thread \".*\" )?([\\w.$]+)(?:: (.+))?$'\n",
        "        self.code_details_pattern = r'^at ([\\w./@$]+)\\.([\\w<>$]+)\\(([\\w.]+):?(\\d+)?\\)'\n",
        "\n",
        "    def parse(self, stack_trace):\n",
        "        \"\"\"\n",
        "        Parses a stack trace string and extracts elements such as exception type, exception message,\n",
        "        class, method, file, and line number.\n",
        "\n",
        "        Parameters:\n",
        "            stack_trace (str): The stack trace string to be parsed.\n",
        "\n",
        "        Returns:\n",
        "            parsed_elements (list): A list of tuples representing parsed elements from the stack trace.\n",
        "        \"\"\"\n",
        "        lines = stack_trace.strip().split('\\n')\n",
        "        parsed_elements = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            exception_match = re.match(self.exception_pattern, line)\n",
        "            if exception_match:\n",
        "                parsed_elements.append(('ExceptionType', exception_match.group(1)))\n",
        "                if exception_match.group(2):\n",
        "                    parsed_elements.append(('ExceptionMessage', exception_match.group(2)))\n",
        "            else:\n",
        "                code_match = re.match(self.code_details_pattern, line)\n",
        "                if code_match:\n",
        "                    class_method = code_match.group(1)\n",
        "                    method = code_match.group(2)\n",
        "                    file = code_match.group(3)\n",
        "                    line_num = code_match.group(4)\n",
        "\n",
        "                    parsed_elements.append(('ClassElem', class_method))\n",
        "                    if method:\n",
        "                        parsed_elements.append(('MethodElem', method))\n",
        "                    if file:\n",
        "                        parsed_elements.append(('FileElem', file))\n",
        "                    if line_num:\n",
        "                        parsed_elements.append(('LineElem', line_num))\n",
        "\n",
        "        return parsed_elements\n",
        "\n",
        "\n",
        "def calculate_accuracy(parsed_elements, expected_elements):\n",
        "    \"\"\"\n",
        "    Calculates the accuracy of parsed elements by comparing them with expected elements.\n",
        "\n",
        "    Parameters:\n",
        "        parsed_elements (list): List of parsed elements from the stack trace.\n",
        "        expected_elements (list): List of expected elements for comparison.\n",
        "\n",
        "    Returns:\n",
        "        accuracy (float): The accuracy of the parsed elements.\n",
        "    \"\"\"\n",
        "    correct_elements = sum(1 for parsed in parsed_elements if parsed in expected_elements)\n",
        "    total_elements = len(expected_elements)\n",
        "    return correct_elements / total_elements if total_elements > 0 else 0\n",
        "\n",
        "\n",
        "def calculate_precision_recall(parsed_elements, expected_elements):\n",
        "    \"\"\"\n",
        "    Calculates the precision and recall of parsed elements.\n",
        "\n",
        "    Parameters:\n",
        "        parsed_elements (list): List of parsed elements from the stack trace.\n",
        "        expected_elements (list): List of expected elements for comparison.\n",
        "\n",
        "    Returns:\n",
        "        precision (float): The precision of the parsed elements.\n",
        "        recall (float): The recall of the parsed elements.\n",
        "    \"\"\"\n",
        "    correct_elements = sum(1 for parsed in parsed_elements if parsed in expected_elements)\n",
        "    precision = correct_elements / len(parsed_elements) if parsed_elements else 0\n",
        "    recall = correct_elements / len(expected_elements) if expected_elements else 0\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "def calculate_f1_score(precision, recall):\n",
        "    \"\"\"\n",
        "    Calculates the F1 score based on precision and recall.\n",
        "\n",
        "    Parameters:\n",
        "        precision (float): The precision value.\n",
        "        recall (float): The recall value.\n",
        "\n",
        "    Returns:\n",
        "        f1_score (float): The F1 score.\n",
        "    \"\"\"\n",
        "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "\n",
        "def process_csv(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Processes a CSV file containing stack traces and expected elements, calculates precision, recall,\n",
        "    and F1 scores for each entry, and writes the results to an output CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        input_file (str): The input CSV file containing stack traces and expected elements.\n",
        "        output_file (str): The output CSV file to write the results.\n",
        "\n",
        "    Returns:\n",
        "        precisions (list): List of precision values for each stack trace.\n",
        "        recalls (list): List of recall values for each stack trace.\n",
        "        f1_scores (list): List of F1 scores for each stack trace.\n",
        "    \"\"\"\n",
        "    parser = StackTraceParser()\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    with open(input_file, mode='r', newline='') as infile, open(output_file, mode='w', newline='') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "        writer.writerow(['Stack Trace', 'Precision', 'Recall', 'F1 Score'])  # Write header\n",
        "        next(reader)  # Skip header\n",
        "\n",
        "        for row in reader:\n",
        "            stack_trace = row[0].strip()\n",
        "            expected_elements_str = row[1].strip()\n",
        "\n",
        "            # Convert expected elements string to list of tuples\n",
        "            expected_elements = eval(expected_elements_str)\n",
        "\n",
        "            parsed_elements = parser.parse(stack_trace)\n",
        "            precision, recall = calculate_precision_recall(parsed_elements, expected_elements)\n",
        "            f1_score = calculate_f1_score(precision, recall)\n",
        "\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            f1_scores.append(f1_score)\n",
        "\n",
        "            print(\"Parsed Elements:\")\n",
        "            for elem in parsed_elements:\n",
        "                print(f\"  {elem[0]}: {elem[1]}\")\n",
        "\n",
        "            print(f\"\\nPrecision: {precision:.2%}\")\n",
        "            print(f\"Recall: {recall:.2%}\")\n",
        "            print(f\"F1 Score: {f1_score:.2%}\\n\")\n",
        "\n",
        "            # Write precision, recall, and F1 score to file\n",
        "            writer.writerow([stack_trace, f\"{precision:.2%}\", f\"{recall:.2%}\", f\"{f1_score:.2%}\"])\n",
        "\n",
        "    return precisions, recalls, f1_scores\n",
        "\n",
        "\n",
        "# File paths\n",
        "#input_file =\n",
        "output_file = 'metrics.csv'\n",
        "\n",
        "# Process CSV and calculate metrics\n",
        "precisions, recalls, f1_scores = process_csv(input_file, output_file)\n",
        "\n",
        "# Calculate average metrics\n",
        "average_precision = sum(precisions) / len(precisions) if precisions else 0\n",
        "average_recall = sum(recalls) / len(recalls) if recalls else 0\n",
        "average_f1_score = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
        "\n",
        "print(f\"Average Precision: {average_precision:.2%}\")\n",
        "print(f\"Average Recall: {average_recall:.2%}\")\n",
        "print(f\"Average F1 Score: {average_f1_score:.2%}\")\n",
        "print(f\"Individual metrics have been saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "W3wN7Tf5vYm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}